{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the weights and bias parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_init = 785.1811367994083\n",
    "w_init = np.array([0.39133535, 18.75376741, -53.36032453, -26.42131618])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_loop(x, w, b):\n",
    "    \"\"\"single predict using linear regression\n",
    "\n",
    "    Args:\n",
    "        x (ndarray): Shape (n,) example with multiple features\n",
    "        w (ndarray): Shape (n,) model parameters\n",
    "        b (scalar): model parameter\n",
    "        \n",
    "    Returns:\n",
    "        p (scalar): prediction\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    p=0\n",
    "    for i in range(n):\n",
    "        p_i = x[i] * w[i]\n",
    "        p = p + p_i\n",
    "    p = p + b\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_single_loop(X_train, w_init, b_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vec = X_train[0,:]\n",
    "f_wb = predict_single_loop(x_vec, w_init, b_init)\n",
    "print(f_wb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(x_vec, w_init) + b_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b):\n",
    "    \"\"\"single predict using linear regression\n",
    "\n",
    "    Args:\n",
    "        x (ndarray): Shape (n,) example with multiple features\n",
    "        w (ndarray): Shape (n,) model parameters\n",
    "        b (scalar): model parameter\n",
    "        \n",
    "    Return:\n",
    "        p (scalar): prediction\n",
    "    \"\"\"\n",
    "    p = np.dot(x, w) + b\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(x_vec, w_init, b_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Cost with Multiple Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b):\n",
    "    \"\"\"compute cost\n",
    "\n",
    "    Args:\n",
    "        X (ndarray (m,n)): Data, m examples with n features\n",
    "        y (ndarray (m,)): target values\n",
    "        w (ndarray (n,)): model parameters\n",
    "        b (scalar): model parameter\n",
    "        \n",
    "    Returns:\n",
    "        cost(scalar): cost\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):\n",
    "        y_hat = np.dot(X[i], w) + b\n",
    "        cost = cost + (y_hat - y[i])**2\n",
    "    cost = cost/(2*m)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = compute_cost(X_train, y_train, w_init, b_init)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute gradient with multiple Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b):\n",
    "    \"\"\"Computes the gradient for linear regression\n",
    "\n",
    "    Args:\n",
    "        X (X (ndarray (m, n))): Data, m examples with n features\n",
    "        y (ndarray (m,)): target values\n",
    "        w (ndarray (n,)): model parameters\n",
    "        b (scalar): model paramerter\n",
    "        \n",
    "    Returns:\n",
    "        dj_dw : (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.\n",
    "        dj_db (scalar):         The gradient of the cost w.r.t. the parameter b.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "    \n",
    "    print(dj_dw)\n",
    "    for i in range(m):\n",
    "        err = (np.dot(X[i],w) + b) - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j]\n",
    "            print(dj_dw)\n",
    "        dj_db = dj_db + err\n",
    "        \n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n",
      "[-0.00500876  0.          0.          0.        ]\n",
      "[-5.00876493e-03 -1.19029585e-05  0.00000000e+00  0.00000000e+00]\n",
      "[-5.00876493e-03 -1.19029585e-05 -2.38059170e-06  0.00000000e+00]\n",
      "[-5.00876493e-03 -1.19029585e-05 -2.38059170e-06 -1.07126626e-04]\n",
      "[-7.31768305e-03 -1.19029585e-05 -2.38059170e-06 -1.07126626e-04]\n",
      "[-7.31768305e-03 -1.67947342e-05 -2.38059170e-06 -1.07126626e-04]\n",
      "[-7.31768305e-03 -1.67947342e-05 -5.64177549e-06 -1.07126626e-04]\n",
      "[-7.31768305e-03 -1.67947342e-05 -5.64177549e-06 -1.72350302e-04]\n",
      "[-8.17870722e-03 -1.67947342e-05 -5.64177549e-06 -1.72350302e-04]\n",
      "[-8.17870722e-03 -1.88159177e-05 -5.64177549e-06 -1.72350302e-04]\n",
      "[-8.17870722e-03 -1.88159177e-05 -6.65236723e-06 -1.72350302e-04]\n",
      "[-8.17870722e-03 -1.88159177e-05 -6.65236723e-06 -2.07721013e-04]\n",
      "-1.6739251122999121e-06\n",
      "[-2.72623574e-03 -6.27197255e-06 -2.21745574e-06 -6.92403377e-05]\n"
     ]
    }
   ],
   "source": [
    "tmp_dj_db, tmp_dj_dw =  compute_gradient(X_train, y_train, w_init, b_init)\n",
    "print(tmp_dj_db)\n",
    "print(tmp_dj_dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent with Multiple Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math\n",
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):\n",
    "    \"\"\"Performs batch gradient descent to learn theta. Updates theta by taking\n",
    "    num_iters gradient steps with learning rate alpha\n",
    "\n",
    "    Args:\n",
    "        X (ndarray(m,n)): Data, m examples with n features\n",
    "        y (ndarray (m,)): target values\n",
    "        w_in (ndarray (n,)): initial model parameters\n",
    "        b_in (scalar): initial model\n",
    "        cost_function: function to compute cost\n",
    "        gradient_function: function to compute the gradient\n",
    "        alpha (float): learning rate\n",
    "        num_iters (int): number of iterations to run gradient descent\n",
    "        \n",
    "    Returns:\n",
    "        w (ndarray(n,)): Updated values of parameters\n",
    "        b (scalar): Updated value of parameter\n",
    "    \"\"\"\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in) # avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        \n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = gradient_function(X, y, w, b)\n",
    "        \n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw \n",
    "        b = b - alpha * dj_db\n",
    "        \n",
    "        #Save cost J at each iteration\n",
    "        if i < 100000:\n",
    "            J_history.append(cost_function(X, y, w, b))\n",
    "            \n",
    "        # Print cost at every intervals 10 times or as many iterations if < 10\n",
    "        if math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
    "            \n",
    "    return w, b, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.\n",
    "iterations = 1000\n",
    "alpha = 5.0e-7\n",
    "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b, compute_cost, compute_gradient, alpha, iterations)\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f}, {w_final}\")\n",
    "m, _ = X_train.shape\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "se_env",
   "language": "python",
   "name": "se_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
