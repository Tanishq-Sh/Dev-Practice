{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to predict the values of y given w, b and x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b):\n",
    "    \"\"\"Predicts value of output based on the input data row, features weights and bias\n",
    "\n",
    "    Args:\n",
    "        x (ndarray (m,)): Example row of input\n",
    "        w (ndarray (n,)): input parameters\n",
    "        b (scalar): input parameter\n",
    "        \n",
    "    Returns:\n",
    "        y (scalar): predicted value\n",
    "    \"\"\"\n",
    "    y_hat = np.dot(x, w) + b\n",
    "    return y_hat\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual : 460, Predicted : 229703\n",
      "Actual : 232, Predicted : 158803\n",
      "Actual : 178, Predicted : 99903\n"
     ]
    }
   ],
   "source": [
    "w_init = np.array([100, 200, 300, 400])\n",
    "b_init = 3\n",
    "m = X_train.shape[0]\n",
    "for i in range(m):\n",
    "    y_hat = predict(X_train[i], w_init, b_init)\n",
    "    print(f\"\"\"Actual : {y_train[i]}, Predicted : {y_hat}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the cost, given the weights and bias of the prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b):\n",
    "    \"\"\"Computes the cost, given the weights and bias of the function\n",
    "\n",
    "    Args:\n",
    "        X (ndarray (m,n)): Training data\n",
    "        y (ndarray (m,)): Target Value\n",
    "        w (ndarray (n,)): Input parameters\n",
    "        b (scalar): Input parameter\n",
    "        \n",
    "    Return:\n",
    "        J (scalar): Cost\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        err = predict(X[i], w, b) - y[i]\n",
    "        cost += err ** 2\n",
    "    cost = cost / (2*m)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost : 14607031785.833334\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Cost : {compute_cost(X_train, y_train, w_init, b_init)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since the cost is high, we need to find the right values of weights and bias to reduce the cost. That's where gradient descent comes in picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Computing the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_gradients(X, y, w, b):\n",
    "    \"\"\"Computes the gradient, given the weights and bias of the function\n",
    "\n",
    "    Args:\n",
    "    X (ndarray (m,n)): Training data\n",
    "    y (ndarray (m,)): Target Value\n",
    "    w (ndarray (n,)): Input parameters\n",
    "    b (scalar): Input parameter\n",
    "\n",
    "    Return:\n",
    "    dj_dw (ndarray (n,)): Gradient w.r.t. w\n",
    "    dj_db (scalar): Gradient w.r.t. b\n",
    "    \"\"\"   \n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    dj_dw = np.zeros(n)\n",
    "    dj_db = 0\n",
    "    for i in range(m):\n",
    "        err = predict(X[i], w, b) - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += (predict(X[i], w, b) - y[i]) * X[i, j]\n",
    "        dj_db += (predict(X[i], w, b) - y[i])\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "    \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-2.72623574e-03, -6.27197255e-06, -2.21745574e-06, -6.92403377e-05]), np.float64(-1.6739251122999121e-06))\n"
     ]
    }
   ],
   "source": [
    "b_init = 785.1811367994083\n",
    "w_init = np.array([0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "print(find_gradients(X_train, y_train, w_init, b_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Now we can implement gradient descent to get the optimal values of w and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "se_env",
   "language": "python",
   "name": "se_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
