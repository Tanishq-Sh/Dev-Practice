{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to predict the values of y given w, b and x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b):\n",
    "    \"\"\"Predicts value of output based on the input data row, features weights and bias\n",
    "\n",
    "    Args:\n",
    "        x (ndarray (m,)): Example row of input\n",
    "        w (ndarray (n,)): input parameters\n",
    "        b (scalar): input parameter\n",
    "        \n",
    "    Returns:\n",
    "        y (scalar): predicted value\n",
    "    \"\"\"\n",
    "    y_hat = np.dot(x, w) + b\n",
    "    return y_hat\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual : 460, Predicted : 229703\n",
      "Actual : 232, Predicted : 158803\n",
      "Actual : 178, Predicted : 99903\n"
     ]
    }
   ],
   "source": [
    "w_init = np.array([100, 200, 300, 400])\n",
    "b_init = 3\n",
    "m = X_train.shape[0]\n",
    "for i in range(m):\n",
    "    y_hat = predict(X_train[i], w_init, b_init)\n",
    "    print(f\"\"\"Actual : {y_train[i]}, Predicted : {y_hat}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the cost, given the weights and bias of the prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b):\n",
    "    \"\"\"Computes the cost, given the weights and bias of the function\n",
    "\n",
    "    Args:\n",
    "        X (ndarray (m,n)): Training data\n",
    "        y (ndarray (m,)): Target Value\n",
    "        w (ndarray (n,)): Input parameters\n",
    "        b (scalar): Input parameter\n",
    "        \n",
    "    Return:\n",
    "        J (scalar): Cost\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        err = predict(X[i], w, b) - y[i]\n",
    "        cost += err ** 2\n",
    "    cost = cost / (2*m)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost : 14607031785.833334\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Cost : {compute_cost(X_train, y_train, w_init, b_init)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since the cost is high, we need to find the right values of weights and bias to reduce the cost. That's where gradient descent comes in picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Computing the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_gradients(X, y, w, b):\n",
    "    \"\"\"Computes the gradient, given the weights and bias of the function\n",
    "\n",
    "    Args:\n",
    "    X (ndarray (m,n)): Training data\n",
    "    y (ndarray (m,)): Target Value\n",
    "    w (ndarray (n,)): Input parameters\n",
    "    b (scalar): Input parameter\n",
    "\n",
    "    Return:\n",
    "    dj_dw (ndarray (n,)): Gradient w.r.t. w\n",
    "    dj_db (scalar): Gradient w.r.t. b\n",
    "    \"\"\"   \n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    dj_dw = np.zeros(n)\n",
    "    dj_db = 0\n",
    "    for i in range(m):\n",
    "        err = predict(X[i], w, b) - y[i]\n",
    "        for j in range(n):\n",
    "            print(dj_dw)\n",
    "            dj_dw[j] += (predict(X[i], w, b) - y[i]) * X[i, j]\n",
    "        dj_db += (predict(X[i], w, b) - y[i])\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "    \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n",
      "[-0.00500876  0.          0.          0.        ]\n",
      "[-5.00876493e-03 -1.19029585e-05  0.00000000e+00  0.00000000e+00]\n",
      "[-5.00876493e-03 -1.19029585e-05 -2.38059170e-06  0.00000000e+00]\n",
      "[-5.00876493e-03 -1.19029585e-05 -2.38059170e-06 -1.07126626e-04]\n",
      "[-7.31768305e-03 -1.19029585e-05 -2.38059170e-06 -1.07126626e-04]\n",
      "[-7.31768305e-03 -1.67947342e-05 -2.38059170e-06 -1.07126626e-04]\n",
      "[-7.31768305e-03 -1.67947342e-05 -5.64177549e-06 -1.07126626e-04]\n",
      "[-7.31768305e-03 -1.67947342e-05 -5.64177549e-06 -1.72350302e-04]\n",
      "[-8.17870722e-03 -1.67947342e-05 -5.64177549e-06 -1.72350302e-04]\n",
      "[-8.17870722e-03 -1.88159177e-05 -5.64177549e-06 -1.72350302e-04]\n",
      "[-8.17870722e-03 -1.88159177e-05 -6.65236723e-06 -1.72350302e-04]\n",
      "(array([-2.72623574e-03, -6.27197255e-06, -2.21745574e-06, -6.92403377e-05]), np.float64(-1.6739251122999121e-06))\n"
     ]
    }
   ],
   "source": [
    "b_init = 785.1811367994083\n",
    "w_init = np.array([0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "print(find_gradients(X_train, y_train, w_init, b_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Now we can implement gradient descent to get the optimal values of w and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w, b, alpha, n_iters):\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    w_copy = w.copy()\n",
    "    b_copy = b\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        dj_dw, dj_db = find_gradients(X, y, w_copy, b_copy)\n",
    "        print(dj_dw)\n",
    "        w_copy = w_copy - alpha * dj_dw\n",
    "        b_copy = b_copy - alpha * dj_db\n",
    "        \n",
    "    return w_copy, b_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.72623574e-03 -6.27197255e-06 -2.21745574e-06 -6.92403377e-05]\n",
      "[-2.72607306e-03 -6.27159818e-06 -2.21732421e-06 -6.92362210e-05]\n",
      "[-2.72591049e-03 -6.27122408e-06 -2.21719275e-06 -6.92321072e-05]\n",
      "[-2.72574778e-03 -6.27084963e-06 -2.21706118e-06 -6.92279891e-05]\n",
      "[-2.72558514e-03 -6.27047533e-06 -2.21692968e-06 -6.92238736e-05]\n",
      "[-2.72542262e-03 -6.27010134e-06 -2.21679822e-06 -6.92197602e-05]\n",
      "[-2.72526007e-03 -6.26972727e-06 -2.21666680e-06 -6.92156467e-05]\n",
      "[-2.72509764e-03 -6.26935347e-06 -2.21653545e-06 -6.92115361e-05]\n",
      "[-2.72493507e-03 -6.26897937e-06 -2.21640399e-06 -6.92074224e-05]\n",
      "[-2.72477264e-03 -6.26860556e-06 -2.21627264e-06 -6.92033118e-05]\n",
      "(array([  0.39133535,  18.75376741, -53.36032453, -26.42131618]), np.float64(785.1811367994083))\n"
     ]
    }
   ],
   "source": [
    "print(gradient_descent(X_train, y_train, w_init, b_init, 0.000000000025, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I Made it Work!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "se_env",
   "language": "python",
   "name": "se_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
